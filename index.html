<!doctype html>
<html lang="en">

<head>
    <title>AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation</title>
    <link rel="icon" type="image/x-icon" href="/static/img/icons/agile_icon.png">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Open Graph -->
    <meta property="og:url" content="https://agile.github.io/" />
    <meta property="og:image" content="https://agile.github.io/static/img/AGILE.png" />
    <meta property="og:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta property="og:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta property="og:description"
        content="A framework that reconstructs simulation-ready interaction sequences from monocular video." />

    <!-- Twitter -->
    <meta name="twitter:url" content="https://agile.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="https://agile.github.io/static/img/AGILE.png" />
    <meta name="twitter:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta name="twitter:description"
        content="A framework that reconstructs simulation-ready interaction sequences from monocular video." />

    <script src="./static/js/distill_template.v2.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
              argmin: '\\operatorname*{arg\\,min}',
              argmax: '\\operatorname*{arg\\,max}'
            }
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://d3js.org/d3-collection.v1.min.js"></script>
    <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

    <script defer="" src="./static/js/hider.js"></script>
    <script src="./static/js/image_interact.js"></script>
    <script src="./static/js/switch_videos.js"></script>

    <link rel="stylesheet" href="./static/css/style.css">
    <link rel="stylesheet" href="./static/css/rerun-viewer.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script defer src="./static/js/fontawesome.all.min.js"></script>


    <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script> <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RY5G0G0QKD"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-RY5G0G0QKD');
    </script>
</head>

<body>

    <div class="header-wrapper">
        <div class="header-container" id="header-container">
            <div class="header-content">
                <h1 style="margin-top: 0px">AGILE</h1>
                <h2>Hand-object Interaction Reconstruction from Video via Agentic Generation</h2>
                <p>
                    Introducing AGILE, a framework that reconstructs simulation-ready interaction
                    sequences from monocular video.
                </p>

                <div class="icon-container">
                    <div class="icon-item">
                        <img src="./static/img/icons/formula.svg" alt="Visual Representation Icon">
                        <div><strong>Novel Generativion Pipeline</strong>: propose the first agentic HOI pipeline that integrates
                            VLM-guided quality assessment with generative models,
                            enabling the production of high-fidelity, watertight meshes
                            independent of video occlusions. 
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/camera.svg" alt="Connector Design Icon">
                        <div><strong>Robust Optimization Strategy</strong>: we introduce
                            a robust anchor-and-track optimization strategy that elim-
                            inates the dependency on brittle SfM by anchoring pose
                            initialization at a single contact frame and propagating it
                            via semantic and geometric alignment.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/cube.svg" alt="Instruction Tuning Data Icon">
                        <div><strong>Physics-Aware Constraints</strong>: We incorporate
                            physics-aware constraints to strictly enforce interaction stability
                            and prevent penetration
                            </p>
                        </div>
                    </div>
                </div>

            </div>
            <!-- header image (intentionally left blank; teaser video placed above Abstract) -->
            <!-- <div class="header-image">
                <img draggable="false" src="static/img/AGILE.png" alt="Teaser Image" class="teaser-image">
            </div> -->
        </div>
    </div>
    <d-article>

        <div class="teaser-caption">
            <div class="rounded" style="text-align: center; padding-bottom: 0px; padding-top: 5px;">
                <div style="text-align: center; color:rgb(0, 0, 0); margin-top: 10px; margin-bottom: 10px;"><strong>TL;DR</strong>: Create <i>canonically posed</i> 3D objects and <i>object-centric cameras</i> from any images in a few forward steps.</div>
            </div>
        </div>

        <!-- Abstract Section -->
        <h1 class="text">Abstract</h1>
        <p class="text abstract">
        Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior arts frequently collapse. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.
        </p> 

        <!-- Demo video
        <h1 class="text">Demo Video</h1>
        <div class="teaser-container">
            <video id="agile-demo" poster="static/AGILE_teaser_frame.png" class="teaser-video rounded" autoplay loop muted controls playsinline preload="metadata">
                <source src="static/AGILE_video.mp4" type="video/mp4">
                Your browser does not support the video tag. <a href="static/AGILE_video.mp4">Download the teaser</a>.
            </video>
        </div> -->

        <!-- <hr> -->
        <div id='interactive_demo' class="interactive-demo-block">
            <div id="sec:interactive_demo" class="sub-section">
                <h1 class="text">Interactive 3D Visualization</h1>
                <div class="rerun-container">
                    <!-- <h3 style="text-align: center;">Interactive Results</h3> -->
                    <div class="viewer-tip-panel" role="note" aria-label="Viewer tip" style="margin-top:0.6em; margin-bottom:22px; display:flex; justify-content:center;">
                        <div style="max-width:840px; width:100%; background: #f0f8ff; border-left: 5px solid #7fb4ff; border-radius:10px; padding:12px 16px; box-shadow: 0 4px 12px rgba(66,133,244,0.06);">
                            <div style="display:flex; gap:12px; align-items:flex-start;">
                                <!-- camera icon -->
                                <div aria-hidden="true" style="flex:0 0 36px; display:flex; align-items:center; justify-content:center;">
                                    <svg width="28" height="28" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="display:block;">
                                        <rect x="2" y="6" width="20" height="14" rx="2" fill="#eaf6ff"></rect>
                                        <path d="M8 6l1.2-2h5.6L16 6" fill="#d9f0ff"></path>
                                        <circle cx="12" cy="13" r="3" fill="#2b6fb3"></circle>
                                    </svg>
                                </div>
                                <div style="flex:1">
                                    <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
                                        <strong style="color:#2b6fb3; font-size:1em;">Tip</strong>
                                        <span style="font-size:0.92em; color:#485f7a;">How to inspect camera views</span>
                                    </div>
                                    <p class="viewer-tip" style="margin:0; color:#23303f; font-size:0.96em; line-height:1.4;">
                                        Double-click to reset the view. Then open the control panel (top-left) and use the <strong>Image Opacity</strong> slider to compare the camera image with the 3D model.
                                        <br>
                                        <small style="color:#657789;">Scene / multi-view examples are available at the end of the thumbnail list.</small>
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div id="viewer"></div>
                    
                    <div class="rerun-thumbnails" id="rerun-thumbnails">
                        <!-- Thumbnails will be generated dynamically from recordings -->
                    </div>
                    
                </div>
            </div>

            <script type="importmap">
            {
                "imports": {
                    "three": "https://unpkg.com/three@0.158.0/build/three.module.js",
                    "three/addons/": "https://unpkg.com/three@0.158.0/examples/jsm/"
                }
            }
            </script>

            <script type="module">
                import { initDemoViewer } from './static/js/demo.js';

                // Build thumbnailList for demo.js
                // 这里是所有 results 的列表
                const thumbnailList = [
                    // { metadataPath: 'static/results/david-kristianto-woqky4GR2CY-unsplash/metadata.json', thumbnail: 'static/results/david-kristianto-woqky4GR2CY-unsplash/images_crop/input_no_mask.png' },
                    // { metadataPath: 'static/results/maria-kovalets-o9mXWquSfk8-unsplash/metadata.json', thumbnail: 'static/results/maria-kovalets-o9mXWquSfk8-unsplash/images_crop/input_no_mask.png' },
                    { metadataPath: 'static/results/SM2/metadata.json', thumbnail: 'static/results/SM2/thumbnail.png' },
                    { metadataPath: 'static/results/GSF13/metadata.json', thumbnail: 'static/results/GSF13/thumbnail.png' },
                    { metadataPath: 'static/results/ABF12/metadata.json', thumbnail: 'static/results/ABF12/thumbnail.png' },
                    
                ];
                
                // Initialize the modular Three.js viewer
                initDemoViewer({
                    containerId: 'viewer',
                    galleryId: 'rerun-thumbnails',
                    thumbnailList
                });
            </script>
        </div>


        <div id='method' class="vision-block">
            <div id="sec:method" class="sub-section">
                <h1 class="text">How AGILE Works?</h1>

                <!-- <p class="text">
                    Reconstructing a 3D object from a single 2D image is a classic challenge. A key difficulty is determining the camera's viewpoint. Our method, <strong>Cupid</strong>, introduces a novel solution: we don't just generate the 3D object, we <strong>jointly generate the object and its camera pose</strong> at the same time.
                </p> -->
                <!-- <h2 class="text">Method Overview</h2> -->
                <p class="text">
                    We use a powerful generative technique called Flow Model<d-cite
                        key="lipman2022flow"></d-cite>. This model learns to transform random noise into a voxelized 3D representation, guided by the input image. To make this process efficient and effective, we break it down into two main stages<d-cite
                        key="xiang2025structured"></d-cite>, as shown in <a href="#fig-method">Figure 1</a>.
                </p>
                
                <p class="text">
                    <strong>Stage 1: Occupancy and Pose Generation. </strong>  
                    The first stage generates a coarse representation of the object and simultaneously estimates the camera pose. Given an input image, our flow model produces two key outputs: an occupancy cube (indicating which voxels $\mathbf{x}_i$ in space belong to the object) and a novel UV cube (indicating the 2D pixel locations $\mathbf{u}_i$ for each 3D voxel).
                    we can robustly solve for the camera's projection matrix $\mathbf{P}^{*}$ using a classical least-squares solver<d-cite
                        key="abdel2015direct"></d-cite>.
                </p>
                <p class="text" style="text-align: center;">
                    $$
                    \mathbf{P}^{*} = \argmin_{\mathbf{P}} \sum_{i} \big\Vert\pi(\mathbf{P},\mathbf{x}_i) - \mathbf{u}_i\big\Vert^2. \tag{1}
                    $$
                </p>

                                <d-figure id="fig-method">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/agentic_gen.png" alt="Pipeline for Agentic Textured Object Generation.">
                        <figcaption>
                            <strong>Figure 1: Overview of Agentic Textured Object Generation.</strong> A VLM agent first selects informative keyframes from the input video to guide multi-view synthesis. To ensure consistency, a VLM-based critic filters the generated views via rejection sampling. The validated images are then lifted to 3D, followed by automated topology optimization and texture refinement. As highlighted in the bottom-right comparison, this refinement step significantly enhances texture fidelity against the evaluated multi-views, yielding a high-quality, simulation-ready asset.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    <strong>Stage 2: Pose-Aligned Geometry and Appearance Generation. </strong>
                    With the camera pose now known, the second stage generates the fine-grained geometry and appearance. A common problem here is "color drift" and "detail inconsistency", where the 3D model doesn't perfectly match the input image's colors and details. We solve this with a <strong>pose-aligned conditioner</strong> that inject pixel-wise information.
                </p>
                <p class="text">
                    For each voxel in the occupancy cube, we use the calculated pose to find exactly where it lands on the 2D input image. We then sample features (both high-level semantics from DINO and low-level color/texture) from that precise pixel location. These pixel-aligned features are injected directly into the generation process, ensuring the final 3D model has high-fidelity geometry and appearance that is faithful to the input view.
                </p>

                <d-figure id="fig-method">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/pipeline.png" alt="Overview of the Cupid method.">
                        <figcaption>
                            <strong>Figure 2: Overview of Cupid.</strong> Our two-stage process first generates a coarse shape and a novel "UV cube" to determine the camera pose. This pose then guides a second stage to generate high-fidelity geometry and appearance.
                        </figcaption>
                    </figure>
                </d-figure>
                
                

                <h2 class="text">Extension 1: Composing Components for Full Scene Reconstruction</h2>
                <p class="text">
                    Our framework naturally extends to reconstructing entire scenes. We use an object detector (like SAM) to find all objects in an image. Then, we run our reconstruction process on each object independently.
                    Then, using the 3D-2D correspondences our method provides, we align each reconstructed object with a global depth prior (from a model like MoGe<d-cite
                        key="wang2025moge"></d-cite>). This allows us to solve for each object's correct scale and position, composing them into a single, coherent 3D scene.
                </p>
                
                <d-figure id="fig-scene">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/scene.png" alt="Component-aligned scene reconstruction.">
                        <figcaption>
                            <strong>Figure 2: Component-Aligned Scene reconstruction.</strong> By reconstructing each object and then solving for its similarity transformation, we can accurately compose complex 3D scenes.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <h2 class="text">Extension 2: Multi-view Reconstruction</h2>
                <p class="text">
                    Although <strong>Cupid</strong> is trained with single image condition, it can be easily extended to multi-view reconstruction, thanks to the flexibility of our generative framework.
                    Given multiple images of the same object from different angles, we know that the 3D object cube should be the same across all views.
                    Therefore, we can run our flow model for each image, but share the same occupancy latent $\mathbf{X}$ across all views during the iterative flow sampling. This is similar to MultiDiffusion<d-cite
                        key="bar2023multidiffusion"></d-cite> (which average the overlapped pixel regions in 2D during iterative sampling), but in the 3D space. 
                </p>

                <d-figure id="fig-multiview">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/multiview.png" alt="Multi-view conditioning results">
                        <figcaption>
                            <strong>Figure 3: Multi-view conditioning.</strong> When multiple input views are available, we fuse the shared, view-agnostic object latent across flow paths (similar to MultiDiffusion<d-cite key="bar2023multidiffusion"></d-cite>), enabling camera, geometry and texture refinement across all images. <em>Top</em>: inputs; <em>Middle</em>: reconstructed 3D object and camera poses; <em>Bottom</em>: rendered images and geometry.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
        </div>
    </d-article>



    <d-appendix>
        <h3>BibTeX</h3>
        <p class="bibtex">
            @article{huang2025cupid,<br>
                title={CUPID: Generative 3D Reconstruction via Joint Object and Pose Modeling},<br>
                author={Huang, Binbin and Duan, Haobin and Zhao, Yiqun and Zhao, Zibo and Ma, Yi and Gao, Shenghua},<br>
                journal={arXiv preprint arXiv:2510.20776},<br>
                year={2025}<br>
                }
        </p>

        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>

        <h3>Acknowledgement</h3>
        <p>
            We thank <a href="https://www.sainingxie.com/" target="_blank">NYU VisionX</a> and <a href="https://agile.github.io/" target="_blank">Cupid</a> for the nice project page template.
        </p>
    </d-appendix>
    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <!-- <script src="./static/js/nav-bar.js"></script> -->
</body>
</html>
