<!doctype html>
<html lang="en">

<head>
    <title>AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation</title>
    <link rel="icon" type="image/x-icon" href="/static/img/icons/agile_icon.png">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Open Graph -->
    <meta property="og:url" content="https://agile-hoi.github.io/" />
    <meta property="og:image" content="https://agile-hoi.github.io/static/img/AGILE.png" />
    <meta property="og:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta property="og:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta property="og:description"
        content="A framework that reconstructs simulation-ready interaction sequences from monocular video." />

    <!-- Twitter -->
    <meta name="twitter:url" content="https://agile-hoi.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="https://agile-hoi.github.io/static/img/AGILE.png" />
    <meta name="twitter:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta name="twitter:description"
        content="A framework that reconstructs simulation-ready interaction sequences from monocular video." />

    <script src="./static/js/distill_template.v2.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
              argmin: '\\operatorname*{arg\\,min}',
              argmax: '\\operatorname*{arg\\,max}'
            }
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://d3js.org/d3-collection.v1.min.js"></script>
    <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

    <script defer="" src="./static/js/hider.js"></script>
    <script src="./static/js/image_interact.js"></script>
    <script src="./static/js/switch_videos.js"></script>

    <link rel="stylesheet" href="./static/css/style.css">
    <link rel="stylesheet" href="./static/css/rerun-viewer.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script defer src="./static/js/fontawesome.all.min.js"></script>


    <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script> <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RY5G0G0QKD"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-RY5G0G0QKD');
    </script>

    <!-- Results Visualization Styles -->
    <link rel="stylesheet" href="./static/css/results-visualization.css">
</head>

<body>

    <div class="header-wrapper">
        <div class="header-container" id="header-container">
            <div class="header-content">
                <h1 style="margin-top: 0px">AGILE</h1>
                <h2>Hand-object Interaction Reconstruction from Video via Agentic Generation</h2>
                <p>
                    Introducing AGILE, a framework that reconstructs simulation-ready interaction
                    sequences from monocular video.
                </p>

                <div class="icon-container">
                    <div class="icon-item">
                        <img src="./static/img/icons/formula.svg" alt="Visual Representation Icon">
                        <div><strong>Novel Generation Pipeline</strong>: Propose the first agentic HOI pipeline that integrates
                            VLM-guided quality assessment with generative models,
                            enabling the production of high-fidelity, watertight meshes
                            independent of video occlusions.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/camera.svg" alt="Connector Design Icon">
                        <div><strong>Robust Optimization Strategy</strong>: We introduce
                            a robust anchor-and-track optimization strategy that eliminates the dependency on brittle SfM by anchoring pose
                            initialization at a single contact frame and propagating it
                            via semantic and geometric alignment.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/cube.svg" alt="Instruction Tuning Data Icon">
                        <div><strong>Physics-Aware Constraints</strong>: We incorporate
                            physics-aware constraints to strictly enforce interaction stability
                            and prevent penetration.
                        </div>
                    </div>
                </div>

            </div>
            <!-- header image (intentionally left blank; teaser video placed above Abstract) -->
            <!-- <div class="header-image">
                <img draggable="false" src="static/img/AGILE.png" alt="Teaser Image" class="teaser-image">
            </div> -->
        </div>
    </div>
    <d-article>

        <div class="teaser-caption">
            <div class="rounded" style="text-align: center; padding-bottom: 0px; padding-top: 5px;">
                <div style="text-align: center; color:rgb(0, 0, 0); margin-top: 10px; margin-bottom: 10px;"><strong>TL;DR</strong>: Reconstruct <i>simulation-ready</i> hand-object interaction from monocular video via <i>agentic generation</i> and <i>robust pose tracking</i>.</div>
            </div>
        </div>

        <!-- Abstract Section -->
        <h1 class="text">Abstract</h1>
        <p class="text abstract">
        Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior arts frequently collapse. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.
        </p> 

        <!-- Demo video
        <h1 class="text">Demo Video</h1>
        <div class="teaser-container">
            <video id="agile-demo" poster="static/AGILE_teaser_frame.png" class="teaser-video rounded" autoplay loop muted controls playsinline preload="metadata">
                <source src="static/AGILE_video.mp4" type="video/mp4">
                Your browser does not support the video tag. <a href="static/AGILE_video.mp4">Download the teaser</a>.
            </video>
        </div> -->

        <!-- <hr> -->
        <div id='interactive_demo' class="interactive-demo-block">
            <div id="sec:interactive_demo" class="sub-section">
                <h1 class="text">Interactive 3D Visualization</h1>
                <div class="rerun-container">
                    <!-- <h3 style="text-align: center;">Interactive Results</h3> -->
                    <div class="viewer-tip-panel" role="note" aria-label="Viewer tip" style="margin-top:0.6em; margin-bottom:22px; display:flex; justify-content:center;">
                        <div style="max-width:840px; width:100%; background: #f0f8ff; border-left: 5px solid #7fb4ff; border-radius:10px; padding:12px 16px; box-shadow: 0 4px 12px rgba(66,133,244,0.06);">
                            <div style="display:flex; gap:12px; align-items:flex-start;">
                                <!-- camera icon -->
                                <div aria-hidden="true" style="flex:0 0 36px; display:flex; align-items:center; justify-content:center;">
                                    <svg width="28" height="28" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="display:block;">
                                        <rect x="2" y="6" width="20" height="14" rx="2" fill="#eaf6ff"></rect>
                                        <path d="M8 6l1.2-2h5.6L16 6" fill="#d9f0ff"></path>
                                        <circle cx="12" cy="13" r="3" fill="#2b6fb3"></circle>
                                    </svg>
                                </div>
                                <div style="flex:1">
                                    <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
                                        <strong style="color:#2b6fb3; font-size:1em;">Tip</strong>
                                        <span style="font-size:0.92em; color:#485f7a;">How to inspect camera views</span>
                                    </div>
                                    <p class="viewer-tip" style="margin:0; color:#23303f; font-size:0.96em; line-height:1.4;">
                                        Double-click to reset the view. Then open the control panel (top-left) and use the <strong>Image Opacity</strong> slider to compare the camera image with the 3D model.
                                        <br>
                                        <small style="color:#657789;">Scene / multi-view examples are available at the end of the thumbnail list.</small>
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div id="viewer"></div>
                    
                    <div class="rerun-thumbnails" id="rerun-thumbnails">
                        <!-- Thumbnails will be generated dynamically from recordings -->
                    </div>
                    
                </div>
            </div>

            <script type="importmap">
            {
                "imports": {
                    "three": "https://unpkg.com/three@0.158.0/build/three.module.js",
                    "three/addons/": "https://unpkg.com/three@0.158.0/examples/jsm/"
                }
            }
            </script>

            <script type="module">
                import { initDemoViewer } from './static/js/demo.js';

                // Build thumbnailList for demo.js
                // 这里是所有 results 的列表
                const thumbnailList = [
                    // { metadataPath: 'static/results/david-kristianto-woqky4GR2CY-unsplash/metadata.json', thumbnail: 'static/results/david-kristianto-woqky4GR2CY-unsplash/images_crop/input_no_mask.png' },
                    // { metadataPath: 'static/results/maria-kovalets-o9mXWquSfk8-unsplash/metadata.json', thumbnail: 'static/results/maria-kovalets-o9mXWquSfk8-unsplash/images_crop/input_no_mask.png' },
                    { metadataPath: 'static/results/SM2/metadata.json', thumbnail: 'static/results/SM2/thumbnail.png' },
                    { metadataPath: 'static/results/GSF13/metadata.json', thumbnail: 'static/results/GSF13/thumbnail.png' },
                    { metadataPath: 'static/results/ABF12/metadata.json', thumbnail: 'static/results/ABF12/thumbnail.png' },
                    
                ];
                
                // Initialize the modular Three.js viewer
                initDemoViewer({
                    containerId: 'viewer',
                    galleryId: 'rerun-thumbnails',
                    thumbnailList
                });
            </script>
        </div>


        <div id='method' class="vision-block">
            <div id="sec:method" class="sub-section">
                <h1 class="text">How AGILE Works?</h1>

                <p class="text">
                    Given a monocular video of a hand-object interaction captured by a fixed camera, our goal is to reconstruct the 4D trajectory of both the hand and the object, yielding high-fidelity, simulation-ready 3D assets. Unlike previous methods that rely on neural rendering and fragile Structure-from-Motion (SfM) initialization, AGILE shifts the paradigm from <i>reconstruction</i> to <i>agentic generation</i>, combining VLM-guided supervision with robust pose tracking. Our framework consists of two main stages, as illustrated in <a href="#fig-pipeline">Figure 2</a>.
                </p>

                <h2 class="text">Stage 1: Agentic Textured Object Generation</h2>
                <p class="text">
                    A primary challenge in reconstructing hand-object interactions from monocular video is the severe occlusion of the object by the user's hand. To overcome these limitations, we propose an agentic framework where a VLM supervisor acts as an intelligent critic, bridging the gap between noisy generative priors and rigorous video evidence.
                </p>

                <p class="text">
                    <strong>VLM-Guided Multi-View Synthesis.</strong> As illustrated in <a href="#fig-method">Figure 1</a>, our pipeline operates through a cascade of generative models steered by a VLM agent. Instead of relying on a single frame, the VLM first selects N informative keyframes (typically 1-4) from the input video to maximize viewpoint coverage. These frames prompt an image generation model to synthesize orthogonal views of the object. To ensure these hallucinations faithfully reflect the video content, we introduce a VLM-based critic. The VLM evaluates the consistency between generated views and original video frames, scoring them on geometry, texture, and material correspondence. Through a rejection sampling process, candidates falling below a strict consistency threshold are discarded and re-generated.
                </p>

                                <d-figure id="fig-method">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/agentic_gen.png" alt="Pipeline for Agentic Textured Object Generation.">
                        <figcaption>
                            <strong>Figure 1: Overview of Agentic Textured Object Generation.</strong> A VLM agent first selects informative keyframes from the input video to guide multi-view synthesis. To ensure consistency, a VLM-based critic filters the generated views via rejection sampling. The validated images are then lifted to 3D, followed by automated topology optimization and texture refinement. As highlighted in the bottom-right comparison, this refinement step significantly enhances texture fidelity against the evaluated multi-views, yielding a high-quality, simulation-ready asset.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    <strong>3D Lifting and Mesh Refinement.</strong> The validated multi-view images are processed by a feed-forward 3D generation model to produce an initial mesh. While geometrically plausible, raw outputs often suffer from irregular topology and blurry textures. To address this, we first apply automated retopology and UV unwrapping to create a clean, lightweight mesh optimized for physics simulation. Subsequently, we perform an <i>agentic texture refinement</i> step. The initial texture is enhanced using an image-to-image editing model conditioned on the evaluated high-resolution multi-view images to recover details. Crucially, this process is also supervised by the VLM agent. The VLM critic evaluates the refined texture against the multi-views via rejection sampling, ensuring strict visual fidelity and discarding any hallucinated artifacts. This rigorous quality control yields a simulation-ready asset with photo-realistic appearance and explicit topology.
                </p>

                <h2 class="text">Stage 2: Hand-Object Interaction Optimization</h2>
                <p class="text">
                    To enable robust tracking without relying on brittle Structure-from-Motion, we establish metric initialization using foundation models. We employ MoGe-2 for depth estimation and SAM2 for segmentation. The hand is initialized via WiLoR to predict MANO parameters, with scale recovered through constrained ICP alignment. For the object, we identify the <i>interaction onset frame (IOF)</i>—when the object first exhibits motion—and apply FoundationPose to estimate its initial pose using our generated mesh.
                </p>

                <p class="text">
                    Starting from the IOF, we perform bi-directional online optimization. For each frame, we first refine the hand translation via joint reprojection, then optimize the object pose using a composite loss that combines: (1) mask alignment for silhouette matching, (2) DINO semantic features for robust tracking under occlusion, and (3) <i>interaction stability constraints</i> that penalize penetration and lock the object to the grasping hand, ensuring physically plausible contact throughout the sequence.
                </p>

                <d-figure id="fig-pipeline">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/pipeline.png" alt="Overview of the AGILE method.">
                        <figcaption>
                            <strong>Figure 2: Overview of AGILE.</strong> Our framework processes the input video in three phases: (1) <b>Agentic Generation:</b> A VLM-guided loop extracts keyframes and supervises the synthesis of a watertight, textured object mesh, utilizing rejection sampling to ensure visual fidelity. (2) <b>SfM-Free Initialization:</b> We decouple metric scale and pose. The hand is initialized via WiLoR, while the object pose is anchored at the Interaction Onset Frame (IOF) using a foundation model. (3) <b>Contact-Aware Optimization:</b> A bi-directional tracking process refines the trajectories. We stabilize the hand via geometric alignment and track the object using semantic and interaction constraints to ensure physical plausibility.
                        </figcaption>
                    </figure>
                </d-figure>

                <!-- <h2 class="text">Application: Real-to-Sim Retargeting</h2>
                <p class="text">
                    To validate the physical plausibility of our reconstructions for Embodied AI, we implement a scalable Real-to-Sim pipeline. We import the reconstructed sequence into Isaac Gym and map the human hand motion to a multi-fingered robotic hand (e.g., Shadow) using kinematic optimization, with the agentic-generated mesh serving as a dynamic rigid body.
                </p>

                <p class="text">
                    As illustrated below, AGILE effectively bridges the gap between video and simulation: (1) <b>Simulation-Ready Assets:</b> Our generated meshes are watertight and topologically clean, enabling stable collision detection without manual post-processing. (2) <b>Robust Kinematic Transfer:</b> Crucially, stable grasps are maintained solely through kinematic mapping, without auxiliary reinforcement learning or physics-based correction. This stability serves as a rigorous validation of our reconstruction quality, confirming that AGILE recovers accurate relative poses and contact states. By automating this process, our framework unlocks the potential to curate large-scale manipulation datasets from unconstrained monocular videos for generalist policy learning.
                </p> -->

                <!-- Results Section -->
                <h1 class="text" style="margin-top: 60px;">Results</h1>

                <!-- Visualization Mode Switch -->
                <div class="viz-switch">
                    <button class="viz-btn active" data-viz="reconstruction">Reconstruction Results</button>
                    <button class="viz-btn" data-viz="rotation">3D Rotation</button>
                    <button class="viz-btn" data-viz="retarget">Real-to-Sim Retargeting</button>
                </div>

                <!-- Reconstruction Results -->
                <div class="viz-content active" id="viz-reconstruction">
                    <p class="text" style="text-align: center; margin-bottom: 20px;">
                        High-fidelity reconstruction results on diverse objects from HO3D, DexYCB, and in-the-wild sequences.
                    </p>
                    <video id="main-video-reconstruction" controls autoplay loop muted style="width:150%; margin-left:-20%; max-width: 3000px; border-radius:12px; box-shadow: 0 4px 24px rgba(0,0,0,0.12); object-fit: contain;">
                        <source src="static/videos/ABF12.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="method-labels" id="method-labels-reconstruction">
                        <!-- Labels will be dynamically generated by JavaScript -->
                    </div>
                    <div class="showcase-container">
                        <div class="showcase-nav showcase-nav-left" data-showcase="reconstruction">‹</div>
                        <div class="showcase-bar" id="showcase-reconstruction"></div>
                        <div class="showcase-nav showcase-nav-right" data-showcase="reconstruction">›</div>
                    </div>
                </div>

                <!-- Rotation Visualization -->
                <div class="viz-content" id="viz-rotation">
                    <p class="text" style="text-align: center; margin-bottom: 20px;">
                        360° rotation visualization of reconstructed objects showing complete geometry and texture quality.
                    </p>
                    <video id="main-video-rotation" controls autoplay loop muted style="width:150%; margin-left:-20%; max-width: 3000px; border-radius:12px; box-shadow: 0 4px 24px rgba(0,0,0,0.12); object-fit: contain;">
                        <source src="static/videos/ABF12_rotate.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="showcase-container">
                        <div class="showcase-nav showcase-nav-left" data-showcase="rotation">‹</div>
                        <div class="showcase-bar" id="showcase-rotation"></div>
                        <div class="showcase-nav showcase-nav-right" data-showcase="rotation">›</div>
                    </div>
                </div>

                <!-- Real-to-Sim Retargeting -->
                <div class="viz-content" id="viz-retarget">
                    <p class="text" style="text-align: center; margin-bottom: 20px;">
                        Simulation-ready assets validated through real-to-sim retargeting in Isaac Gym.
                    </p>
                     <video id="main-video-retarget" controls autoplay loop muted style="width:150%; margin-left:-20%; max-width: 3000px; border-radius:12px; box-shadow: 0 4px 24px rgba(0,0,0,0.12); object-fit: contain;">
                        <source src="static/videos/ABF12_retarget.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="showcase-container">
                        <div class="showcase-nav showcase-nav-left" data-showcase="retarget">‹</div>
                        <div class="showcase-bar" id="showcase-retarget"></div>
                        <div class="showcase-nav showcase-nav-right" data-showcase="retarget">›</div>
                    </div>
                </div>

            </div>
        </div>
    </d-article>



    <d-appendix>
        <h3>BibTeX</h3>
        <p class="bibtex">
            @article{agile2026,<br>
                title={AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation},<br>
                    author={Anonymous},<br>
                    journal={Under Review},<br>
                    year={2026}<br>
                }
        </p>

        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>

        <h3>Acknowledgement</h3>
        <p>
            We thank the authors of related works for their nice project page templates and open-source implementations.
        </p>
    </d-appendix>

    <!-- Hidden citations for bibliography (used in dynamically generated content) -->
    <div style="display: none;">
        <d-cite key="fan2024hold"></d-cite>
        <d-cite key="wang2025magichoi"></d-cite>
    </div>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <!-- <script src="./static/js/nav-bar.js"></script> -->

    <!-- Results Visualization Script -->
    <script defer src="./static/js/results-visualization.js"></script>
</body>
</html>
