<!doctype html>
<html lang="en">

<head>
    <title>AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation</title>
    <link rel="icon" type="image/x-icon" href="/static/img/icons/agile_icon.png">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Open Graph -->
    <meta property="og:url" content="https://agile-hoi.github.io/" />
    <meta property="og:image" content="https://agile-hoi.github.io/static/img/AGILE.png" />
    <meta property="og:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta property="og:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta property="og:description"
        content="A framework that reconstructs simulation-ready interaction sequences from monocular video." />

    <!-- Twitter -->
    <meta name="twitter:url" content="https://agile-hoi.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="https://agile-hoi.github.io/static/img/AGILE.png" />
    <meta name="twitter:title" content="AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation" />
    <meta name="twitter:description"
        content="A framework that reconstructs simulation-ready interaction sequences from monocular video." />

    <script src="./static/js/distill_template.v2.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
              argmin: '\\operatorname*{arg\\,min}',
              argmax: '\\operatorname*{arg\\,max}'
            }
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://d3js.org/d3-collection.v1.min.js"></script>
    <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

    <script defer="" src="./static/js/hider.js"></script>
    <script src="./static/js/image_interact.js"></script>
    <script src="./static/js/switch_videos.js"></script>

    <link rel="stylesheet" href="./static/css/style.css">
    <link rel="stylesheet" href="./static/css/rerun-viewer.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script defer src="./static/js/fontawesome.all.min.js"></script>


    <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script> <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RY5G0G0QKD"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-RY5G0G0QKD');
    </script>

    <!-- Results Visualization Styles -->
    <link rel="stylesheet" href="./static/css/results-visualization.css">
    <link rel="stylesheet" href="./static/css/quantitative-results.css">
</head>

<body>

    <div class="header-wrapper">
        <div class="header-container" id="header-container">
            <div class="header-content">
                <h1 style="margin-top: 0px">AGILE</h1>
                <h2>Hand-object Interaction Reconstruction from Video via Agentic Generation</h2>
                <p>
                    Introducing AGILE, a framework that reconstructs simulation-ready interaction
                    sequences from monocular video.
                </p>

                <div class="icon-container">
                    <div class="icon-item">
                        <img src="./static/img/icons/formula.svg" alt="Visual Representation Icon">
                        <div><strong>Novel Generation Pipeline</strong>: We propose the first agentic HOI pipeline that integrates
                            VLM-guided quality assessment with generative models,
                            enabling the production of high-fidelity, watertight meshes
                            independent of video occlusions.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/camera.svg" alt="Connector Design Icon">
                        <div><strong>Robust Optimization Strategy</strong>: We introduce
                            a robust anchor-and-track optimization strategy that eliminates the dependency on brittle SfM by anchoring pose
                            initialization at a single contact frame and propagating it
                            via semantic and geometric alignment.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/cube.svg" alt="Instruction Tuning Data Icon">
                        <div><strong>Physics-Aware Constraints</strong>: We incorporate
                            physics-aware constraints to strictly enforce interaction stability
                            and prevent penetration.
                        </div>
                    </div>
                </div>

            </div>
            <!-- header image (intentionally left blank; teaser video placed above Abstract) -->
            <!-- <div class="header-image">
                <img draggable="false" src="static/img/AGILE.png" alt="Teaser Image" class="teaser-image">
            </div> -->
        </div>
    </div>
    <d-article>

        <div class="teaser-caption">
            <div class="rounded" style="text-align: center; padding-bottom: 0px; padding-top: 5px;">
                <div style="text-align: center; color:rgb(0, 0, 0); margin-top: 10px; margin-bottom: 10px;"><strong>TL;DR</strong>: Reconstruct <i>simulation-ready</i> hand-object interaction from monocular video via <i>agentic generation</i> and <i>robust pose tracking</i>.</div>
            </div>
        </div>

        <!-- Abstract Section -->
        <h1 class="text">Abstract</h1>
        <p class="text abstract">
        Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior arts frequently collapse. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.
        </p> 

        <!-- <hr> -->
        <div id='interactive_demo' class="interactive-demo-block">
            <div id="sec:interactive_demo" class="sub-section">
                <h1 class="text">Interactive 3D Visualization</h1>
                <div class="rerun-container">
                    <!-- <h3 style="text-align: center;">Interactive Results</h3> -->
                    <div class="viewer-tip-panel" role="note" aria-label="Viewer tip" style="margin-top:0.6em; margin-bottom:22px; display:flex; justify-content:center;">
                        <div style="max-width:840px; width:100%; background: #f0f8ff; border-left: 5px solid #7fb4ff; border-radius:10px; padding:12px 16px; box-shadow: 0 4px 12px rgba(66,133,244,0.06);">
                            <div style="display:flex; gap:12px; align-items:flex-start;">
                                <!-- camera icon -->
                                <div aria-hidden="true" style="flex:0 0 36px; display:flex; align-items:center; justify-content:center;">
                                    <svg width="28" height="28" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="display:block;">
                                        <rect x="2" y="6" width="20" height="14" rx="2" fill="#eaf6ff"></rect>
                                        <path d="M8 6l1.2-2h5.6L16 6" fill="#d9f0ff"></path>
                                        <circle cx="12" cy="13" r="3" fill="#2b6fb3"></circle>
                                    </svg>
                                </div>
                                <div style="flex:1">
                                    <div style="display:flex; align-items:center; gap:8px; margin-bottom:6px;">
                                        <strong style="color:#2b6fb3; font-size:1em;">Tip</strong>
                                        <span style="font-size:0.92em; color:#485f7a;">How to inspect camera views</span>
                                    </div>
                                    <p class="viewer-tip" style="margin:0; color:#23303f; font-size:0.96em; line-height:1.4;">
                                        Double-click to reset the view. Use the <strong>play/pause button</strong> (⏸️) to pause the animation and <strong>frame controls</strong> to navigate. Adjust <strong>Opacity</strong> to compare video frames with the 3D hand-object reconstruction.
                                        <br>
                                        <small style="color:#657789;">The purple-blue mesh shows the reconstructed hand, and the object mesh updates dynamically with the video.</small>
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div id="viewer"></div>
                    
                    <div class="rerun-thumbnails" id="rerun-thumbnails">
                        <!-- Thumbnails will be generated dynamically from recordings -->
                    </div>
                    
                </div>
            </div>

            <script type="importmap">
            {
                "imports": {
                    "three": "https://unpkg.com/three@0.158.0/build/three.module.js",
                    "three/addons/": "https://unpkg.com/three@0.158.0/examples/jsm/"
                }
            }
            </script>

            <script type="module">
                import { initDemoViewer } from './static/js/demo.js';

                // Build thumbnailList for demo.js
                // 这里是所有 results 的列表
                const thumbnailList = [
                    // { metadataPath: 'static/results/david-kristianto-woqky4GR2CY-unsplash/metadata.json', thumbnail: 'static/results/david-kristianto-woqky4GR2CY-unsplash/images_crop/input_no_mask.png' },
                    // { metadataPath: 'static/results/maria-kovalets-o9mXWquSfk8-unsplash/metadata.json', thumbnail: 'static/results/maria-kovalets-o9mXWquSfk8-unsplash/images_crop/input_no_mask.png' },
                    { metadataPath: 'static/results/SM2/metadata.json', thumbnail: 'static/results/SM2/thumbnail.png' },
                    { metadataPath: 'static/results/GSF13/metadata.json', thumbnail: 'static/results/GSF13/thumbnail.png' },
                    { metadataPath: 'static/results/ABF12/metadata.json', thumbnail: 'static/results/ABF12/thumbnail.png' },
                    { metadataPath: 'static/results/SMu40/metadata.json', thumbnail: 'static/results/SMu40/thumbnail.png' },
                    { metadataPath: 'static/results/MDF12/metadata.json', thumbnail: 'static/results/MDF12/thumbnail.png' },
                ];
                
                // Initialize the modular Three.js viewer
                initDemoViewer({
                    containerId: 'viewer',
                    galleryId: 'rerun-thumbnails',
                    thumbnailList
                });
            </script>
        </div>


        <div id='method' class="vision-block">
            <div id="sec:method" class="sub-section">
                <h1 class="text">How AGILE Works?</h1>

                <p class="text">
                    Given monocular video, AGILE reconstructs 4D hand-object trajectories with simulation-ready assets. Unlike methods relying on neural rendering and fragile SfM, we shift from <i>reconstruction</i> to <i>agentic generation</i> with VLM-guided supervision and robust pose tracking (<a href="#fig-pipeline">Figure 2</a>).
                </p>

                <h2 class="text">Stage 1: Agentic Textured Object Generation</h2>
                <p class="text">
                    To address severe hand occlusion, we employ an agentic framework where a VLM guides multi-view synthesis and supervises mesh generation via rejection sampling.
                </p>

                <p class="text">
                    <strong>VLM-Guided Multi-View Synthesis.</strong> A VLM agent selects informative keyframes to synthesize orthogonal object views. A VLM critic filters generated images via rejection sampling to ensure consistency with video observations.
                </p>

                <p class="text">
                    <strong>3D Lifting and Mesh Refinement.</strong> Validated views are processed to generate an initial mesh, then refined through automated retopology and VLM-supervised texture enhancement, producing simulation-ready assets with high-fidelity appearance.
                </p>

                <d-figure id="fig-method">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/agentic_gen.png" alt="Pipeline for Agentic Textured Object Generation.">
                        <figcaption>
                            <strong>Figure 1: Agentic Textured Object Generation.</strong> A VLM agent selects keyframes and guides multi-view synthesis with rejection sampling. Validated views are lifted to 3D, followed by topology optimization and VLM-supervised texture refinement.
                        </figcaption>
                    </figure>
                </d-figure>

                <h2 class="text">Stage 2: Hand-Object Interaction Optimization</h2>
                <p class="text">
                    We establish metric initialization using foundation models (MoGe-2 for depth, SAM2 for segmentation). The hand is initialized via WiLoR with scale recovered through ICP. For the object, we identify the interaction onset frame and apply FoundationPose using our generated mesh.
                </p>

                <p class="text">
                    Starting from this frame, we perform bi-directional optimization. For each frame, we refine hand translation via joint reprojection, then optimize object pose using mask alignment, DINO semantic features, and interaction stability constraints that prevent penetration and lock the object to the grasping hand.
                </p>

                <d-figure id="fig-pipeline">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/pipeline.png" alt="Overview of the AGILE method.">
                        <figcaption>
                            <strong>Figure 2: Overview of AGILE.</strong> (1) <b>Agentic Generation:</b> VLM-guided multi-view synthesis with rejection sampling produces a watertight textured mesh. (2) <b>SfM-Free Initialization:</b> Hand initialized via WiLoR, object pose anchored at Interaction Onset Frame. (3) <b>Contact-Aware Optimization:</b> Bi-directional tracking with semantic and interaction constraints.
                        </figcaption>
                    </figure>
                </d-figure>

                <!-- <h2 class="text">Application: Real-to-Sim Retargeting</h2>
                <p class="text">
                    To validate the physical plausibility of our reconstructions for Embodied AI, we implement a scalable Real-to-Sim pipeline. We import the reconstructed sequence into Isaac Gym and map the human hand motion to a multi-fingered robotic hand (e.g., Shadow) using kinematic optimization, with the agentic-generated mesh serving as a dynamic rigid body.
                </p>

                <p class="text">
                    As illustrated below, AGILE effectively bridges the gap between video and simulation: (1) <b>Simulation-Ready Assets:</b> Our generated meshes are watertight and topologically clean, enabling stable collision detection without manual post-processing. (2) <b>Robust Kinematic Transfer:</b> Crucially, stable grasps are maintained solely through kinematic mapping, without auxiliary reinforcement learning or physics-based correction. This stability serves as a rigorous validation of our reconstruction quality, confirming that AGILE recovers accurate relative poses and contact states. By automating this process, our framework unlocks the potential to curate large-scale manipulation datasets from unconstrained monocular videos for generalist policy learning.
                </p> -->

                <!-- Results Section -->
                <h1 class="text" style="margin-top: 60px;">Qualitative Results</h1>

                <!-- Visualization Mode Switch -->
                <div class="viz-switch">
                    <button class="viz-btn active" data-viz="reconstruction">Reconstruction Results</button>
                    <button class="viz-btn" data-viz="rotation">3D Rotation</button>
                    <button class="viz-btn" data-viz="retarget">Real-to-Sim Retargeting</button>
                </div>

                <!-- Reconstruction Results -->
                <div class="viz-content active" id="viz-reconstruction">
                    <p class="text" style="text-align: center; margin-bottom: 20px;">
                        High-fidelity reconstruction results on diverse objects from HO3D-v3<d-cite key="hampali2020honnotate"></d-cite>, DexYCB<d-cite key="chao2021dexycb"></d-cite>, and in-the-wild sequences.
                    </p>
                    <video id="main-video-reconstruction" controls autoplay loop muted style="width:150%; margin-left:-20%; max-width: 3000px; border-radius:12px; box-shadow: 0 4px 24px rgba(0,0,0,0.12); object-fit: contain;">
                        <source src="static/videos/GSF13.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="method-labels" id="method-labels-reconstruction">
                        <!-- Labels will be dynamically generated by JavaScript -->
                    </div>
                    <div class="showcase-container">
                        <div class="showcase-nav showcase-nav-left" data-showcase="reconstruction">‹</div>
                        <div class="showcase-bar" id="showcase-reconstruction"></div>
                        <div class="showcase-nav showcase-nav-right" data-showcase="reconstruction">›</div>
                    </div>
                </div>

                <!-- Rotation Visualization -->
                <div class="viz-content" id="viz-rotation">
                    <p class="text" style="text-align: center; margin-bottom: 20px;">
                        360° rotation visualization of reconstructed objects showing complete geometry and texture quality.
                    </p>
                    <video id="main-video-rotation" controls autoplay loop muted style="width:150%; margin-left:-20%; max-width: 3000px; border-radius:12px; box-shadow: 0 4px 24px rgba(0,0,0,0.12); object-fit: contain;">
                        <source src="static/videos/ABF12_rotate.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="showcase-container">
                        <div class="showcase-nav showcase-nav-left" data-showcase="rotation">‹</div>
                        <div class="showcase-bar" id="showcase-rotation"></div>
                        <div class="showcase-nav showcase-nav-right" data-showcase="rotation">›</div>
                    </div>
                </div>

                <!-- Real-to-Sim Retargeting -->
                <div class="viz-content" id="viz-retarget">
                    <p class="text" style="text-align: center; margin-bottom: 20px;">
                        Simulation-ready assets validated through real-to-sim retargeting in Isaac Gym.
                    </p>
                     <video id="main-video-retarget" controls autoplay loop muted style="width:150%; margin-left:-20%; max-width: 3000px; border-radius:12px; box-shadow: 0 4px 24px rgba(0,0,0,0.12); object-fit: contain;">
                        <source src="static/videos/ABF12_retarget.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="showcase-container">
                        <div class="showcase-nav showcase-nav-left" data-showcase="retarget">‹</div>
                        <div class="showcase-bar" id="showcase-retarget"></div>
                        <div class="showcase-nav showcase-nav-right" data-showcase="retarget">›</div>
                    </div>
                </div>

            </div>
        </div>

        <!-- Quantitative Comparison Section -->
        <div id='quantitative' class="vision-block">
            <div class="sub-section">
                <h1 class="text" style="margin-top: 60px;">Quantitative Results</h1>
                <p class="text">
                    We conduct extensive quantitative evaluation on HO3D-v3<d-cite key="hampali2020honnotate"></d-cite> and DexYCB<d-cite key="chao2021dexycb"></d-cite> datasets,
                    comparing AGILE against state-of-the-art methods HOLD<d-cite key="fan2024hold"></d-cite>
                    and MagicHOI<d-cite key="wang2025magichoi"></d-cite> across
                    multiple metrics including hand pose accuracy (MPJPE), object geometry fidelity (CD, F-scores),
                    interaction quality (CD<sub>h</sub>), and success rate (SR).
                </p>

                <!-- Dataset Toggle -->
                <div class="dataset-switch">
                    <button class="dataset-btn active" data-dataset="ho3d">HO3D-v3</button>
                    <button class="dataset-btn" data-dataset="dexycb">DexYCB</button>
                </div>

                <!-- View Toggle - Hidden -->
                <div class="quant-viz-switch" style="display: none;">
                    <button class="quant-viz-btn active" data-view="table">Table View</button>
                    <button class="quant-viz-btn" data-view="chart">Chart View</button>
                </div>

                <!-- Table View -->
                <div id="table-view" class="quant-view active">
                    <div class="table-container" id="comparison-table"></div>
                </div>

                <!-- Chart View - Hidden -->
                <div id="chart-view" class="quant-view" style="display: none;">
                    <!-- Chart Mode Toggle -->
                    <div class="chart-mode-switch">
                        <button class="chart-mode-btn active" data-mode="individual">By Metric</button>
                        <button class="chart-mode-btn" data-mode="comprehensive">Overall</button>
                    </div>
                    <div id="comparison-chart"></div>
                </div>

                <p class="text note" style="font-size: 0.9em; color: #666; margin-top: 20px;">
                    <strong>Note:</strong> Baseline metrics (<sup>†</sup>) are averaged over successful sequences only (survivor bias).
                    AGILE achieves 100% success rate on both datasets, while HOLD fails on 55% of DexYCB
                    sequences and MagicHOI fails on 75%.
                </p>
            </div>
        </div>
    </d-article>



    <d-appendix>
        <h3>BibTeX</h3>
        <p class="bibtex">
            @article{agile2026,<br>
                title={AGILE: Hand-object Interaction Reconstruction from Video via Agentic Generation},<br>
                    author={Anonymous},<br>
                    journal={Under Review},<br>
                    year={2026}<br>
                }
        </p>

        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>

        <h3>Acknowledgement</h3>
        <p>
            We thank <a href="https://www.sainingxie.com/" target="_blank">NYU VisionX</a> and <a href="https://cupid3d.github.io/" target="_blank">Cupid</a> for the nice project page template.
        </p>
    </d-appendix>

    <!-- Hidden citations for bibliography (used in dynamically generated content) -->
    <div style="display: none;">
        <d-cite key="fan2024hold"></d-cite>
        <d-cite key="wang2025magichoi"></d-cite>
    </div>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <!-- <script src="./static/js/nav-bar.js"></script> -->

    <!-- Results Visualization Script -->
    <script defer src="./static/js/results-visualization.js"></script>

    <!-- Quantitative Results Script -->
    <script defer src="./static/js/quantitative-results.js"></script>
</body>
</html>
